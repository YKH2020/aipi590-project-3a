{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Project 3a: Advanced GAN Crystal Ball__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import datetime\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Torch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchgan.models as models\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from pytorch_fid import fid_score\n",
    "\n",
    "print(dir(models))\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())\n",
    "else:\n",
    "    print('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Download__ and __Extract__ the CelebA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl -L https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/wiki_crop.tar -o wiki_crop.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tar -xvzf wiki_crop.tar -C ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Load the Celeb-WIKI Dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file_path = './wiki_crop/wiki.mat'\n",
    "mat = scipy.io.loadmat(mat_file_path)\n",
    "\n",
    "mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from .mat file\n",
    "wiki = mat['wiki']\n",
    "\n",
    "full_path = wiki['full_path'][0][0][0]\n",
    "gender = wiki['gender'][0][0][0]\n",
    "dob = wiki['dob'][0][0][0]\n",
    "photo_taken = wiki['photo_taken'][0][0][0]\n",
    "face_location = wiki['face_location'][0][0][0]\n",
    "name = wiki['name'][0][0][0]\n",
    "face_score = wiki['face_score'][0][0][0]\n",
    "second_face_score = wiki['second_face_score'][0][0][0]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'full_path': full_path,\n",
    "    'gender': gender.flatten(),\n",
    "    'dob': dob.flatten(),\n",
    "    'photo_taken': photo_taken.flatten(),\n",
    "    'face_location': face_location.tolist(),\n",
    "    'name': name.flatten(),\n",
    "    'face_score': face_score.flatten(),\n",
    "    'second_face_score': second_face_score.flatten()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Cleaning__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -inf for face score means that the confidence of a face being detected in the image is virtually NONEXISTENT!\n",
    "num_neg_inf = (df['face_score'] == -np.inf).sum()\n",
    "print(num_neg_inf)\n",
    "df_filtered = df[df['face_score'] != -np.inf]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nans = df_filtered['second_face_score'].isna().sum()\n",
    "df_filtered = df_filtered[df_filtered['second_face_score'].isna()]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nans = df_filtered['gender'].isna().sum()\n",
    "df_filtered = df_filtered[df_filtered['gender'].notna()]\n",
    "num_nans = df_filtered['gender'].isna().sum()\n",
    "print(num_nans)\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.drop(columns=['second_face_score'])\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matlab_serial_to_year(serial_date):\n",
    "    # MATLAB's serial dates start from 0000-01-01, Python starts from 0001-01-01\n",
    "    origin = datetime.datetime(1, 1, 1)  # Using year 1\n",
    "    delta = datetime.timedelta(days=int(serial_date) - 366)  # Subtract 366 to adjust MATLAB's start year (0)\n",
    "    return (origin + delta).year\n",
    "\n",
    "# Assuming your cleaned DataFrame is named 'df'\n",
    "def get_age_bucket(age):\n",
    "    if age <= 18:\n",
    "        return 1\n",
    "    elif 19 <= age <= 29:\n",
    "        return 2\n",
    "    elif 30 <= age <= 39:\n",
    "        return 3\n",
    "    elif 40 <= age <= 49:\n",
    "        return 4\n",
    "    elif 50 <= age <= 59:\n",
    "        return 5\n",
    "    else:\n",
    "        return 6\n",
    "\n",
    "final_df = df_filtered.copy()\n",
    "final_df['dob'] = df_filtered['dob'].apply(matlab_serial_to_year)\n",
    "\n",
    "# Add another feature\n",
    "final_df['age'] = final_df['photo_taken'] - final_df['dob']\n",
    "final_df = final_df.drop(columns=['dob', 'photo_taken'])\n",
    "\n",
    "# Assign age bucket to each row\n",
    "final_df['age_bucket'] = final_df['age'].apply(get_age_bucket)\n",
    "\n",
    "# Add './wiki_crop/' prefix to the full_path column to get the correct paths\n",
    "final_df['full_path'] = final_df['full_path'].apply(lambda x: f\"./wiki_crop/{x[0]}\")\n",
    "\n",
    "# Convert gender to int\n",
    "final_df['gender'] = final_df['gender'].astype(int)\n",
    "\n",
    "# Check the updated DataFrame\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['face_location'] = final_df['face_location'].apply(lambda x: x[0].tolist() if isinstance(x, np.ndarray) and x.ndim == 2 else x)\n",
    "final_df['name'] = final_df['name'].apply(lambda x: x[0] if isinstance(x, np.ndarray) and x.ndim == 1 else x)\n",
    "\n",
    "print(final_df.dtypes)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Preprocessing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('./final_data.csv')\n",
    "\n",
    "# 1. Parse `face_location`\n",
    "data['face_location'] = data['face_location'].apply(eval)\n",
    "\n",
    "# 2. Verify Image Paths\n",
    "valid_paths = data['full_path'].apply(lambda x: os.path.exists(x))\n",
    "if not valid_paths.all():\n",
    "    print(f\"Invalid paths detected: {data[~valid_paths]}\")\n",
    "    data = data[valid_paths]\n",
    "\n",
    "# 3. Prepare Conditions\n",
    "# One-hot encode age_bucket\n",
    "age_buckets = torch.eye(6)[data['age_bucket'] - 1]  # Zero-indexed buckets (1-6 -> 0-5)\n",
    "# Gender as a tensor\n",
    "genders = torch.tensor(data['gender'].values, dtype=torch.float32).unsqueeze(1)\n",
    "# Combine age_bucket and gender\n",
    "conditions = torch.cat((age_buckets, genders), dim=1)\n",
    "\n",
    "# 4. Transform and Resize Images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Preprocess images\n",
    "images = []\n",
    "valid_image_paths = []\n",
    "\n",
    "for path in data['full_path']:\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = transform(img)\n",
    "        images.append(img)\n",
    "        valid_image_paths.append(path)  # Keep track of valid paths\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {path}: {e}\")\n",
    "\n",
    "# Convert to tensors\n",
    "images = torch.stack(images)\n",
    "\n",
    "# Update the dataframe to include only valid image paths\n",
    "data = data[data['full_path'].isin(valid_image_paths)]\n",
    "\n",
    "print(f\"Images Shape: {images.shape}\")\n",
    "print(f\"Conditions Shape: {conditions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Modeling__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils as nn_utils\n",
    "class MyConditionalGenerator(nn.Module):\n",
    "    def __init__(self, noise_dim, condition_dim, img_channels=3, img_size=64):\n",
    "        super(MyConditionalGenerator, self).__init__()\n",
    "        self.init_size = img_size // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(noise_dim + condition_dim, 256 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 256, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, img_channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.cat((noise, labels), -1)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 256, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConditionalDiscriminator(nn.Module):\n",
    "    def __init__(self, condition_dim, img_channels=3, img_size=64):\n",
    "        super(MyConditionalDiscriminator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.model = nn.Sequential(\n",
    "            nn_utils.spectral_norm(nn.Conv2d(img_channels + condition_dim, 64, 4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn_utils.spectral_norm(nn.Conv2d(64, 128, 4, stride=2, padding=1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn_utils.spectral_norm(nn.Conv2d(128, 256, 4, stride=2, padding=1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn_utils.spectral_norm(nn.Conv2d(256, 512, 4, stride=2, padding=1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        ds_size = img_size // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(\n",
    "            nn_utils.spectral_norm(nn.Linear(512 * ds_size ** 2, 1)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Expand labels to have the same spatial dimensions as the image\n",
    "        labels = labels.view(labels.size(0), labels.size(1), 1, 1)\n",
    "        labels = labels.expand(labels.size(0), labels.size(1), self.img_size, self.img_size)\n",
    "        d_in = torch.cat((img, labels), 1)\n",
    "        out = self.model(d_in)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Combine images and conditions into a dataset\n",
    "dataset = TensorDataset(images, conditions)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Check a batch\n",
    "for batch_imgs, batch_conditions in dataloader:\n",
    "    print(f\"Batch Image Shape: {batch_imgs.shape}, Batch Condition Shape: {batch_conditions.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted Hyperparameters\n",
    "noise_dim = 100\n",
    "condition_dim = conditions.shape[1]\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "lr_G = 0.0002       # Lowered generator learning rate\n",
    "lr_D = 0.00005      # Reduced discriminator learning rate\n",
    "betas = (0.5, 0.999)\n",
    "# Removed 'generator_steps' since we're training the generator once per iteration\n",
    "\n",
    "generator = MyConditionalGenerator(noise_dim, condition_dim).to(device)\n",
    "discriminator = MyConditionalDiscriminator(condition_dim).to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr_G, betas=betas)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr_D, betas=betas)\n",
    "\n",
    "# Define fixed noise and conditions for visualization\n",
    "fixed_noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "fixed_conds = conditions[:batch_size].to(device)  # Ensure batch_size samples\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    for real_imgs, conds in dataloader:\n",
    "        real_imgs, conds = real_imgs.to(device), conds.to(device)\n",
    "        conds_flat = conds.view(conds.size(0), -1)\n",
    "        current_batch_size = real_imgs.size(0)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        # Generate fake images\n",
    "        noise = torch.randn(current_batch_size, noise_dim).to(device)\n",
    "        fake_imgs = generator(noise, conds_flat)\n",
    "\n",
    "        # Real and fake labels with label smoothing\n",
    "        real_labels = torch.full((current_batch_size, 1), 0.9).to(device)  # Real labels smoothed to 0.9\n",
    "        fake_labels = torch.zeros(current_batch_size, 1).to(device)         # Fake labels at 0\n",
    "\n",
    "        # Compute discriminator loss\n",
    "        real_output = discriminator(real_imgs, conds_flat)\n",
    "        fake_output = discriminator(fake_imgs.detach(), conds_flat)\n",
    "        real_loss = loss_fn(real_output, real_labels)\n",
    "        fake_loss = loss_fn(fake_output, fake_labels)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate fake images\n",
    "        noise = torch.randn(current_batch_size, noise_dim).to(device)\n",
    "        fake_imgs = generator(noise, conds_flat)\n",
    "\n",
    "        # Generator tries to fool discriminator\n",
    "        output = discriminator(fake_imgs, conds_flat)\n",
    "        g_loss = loss_fn(output, real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    # Save generated images for visualization\n",
    "    with torch.no_grad():\n",
    "        fake_imgs = generator(fixed_noise, fixed_conds)\n",
    "        vutils.save_image(fake_imgs, f'generated_epoch_{epoch+1}.png', normalize=True)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Testing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ensure your generator is in evaluation mode\n",
    "generator.eval()\n",
    "\n",
    "# 1. Select a Random Image and Extract Its Condition\n",
    "# Get a random index from the dataset\n",
    "random_idx = random.randint(0, len(dataset) - 1)\n",
    "\n",
    "# Retrieve the condition (assuming you don't need the image itself)\n",
    "_, random_condition = dataset[random_idx]\n",
    "\n",
    "# Assuming the condition tensor is structured as [age_one_hot (6), gender (1)]\n",
    "# Extract the gender condition\n",
    "gender_condition = random_condition[-1:]  # Shape: [1]\n",
    "\n",
    "# Convert to the appropriate device\n",
    "gender_condition = gender_condition.to(device)\n",
    "\n",
    "# 2. Fix a Noise Vector\n",
    "fixed_noise = torch.randn(1, noise_dim).to(device)  # Shape: [1, noise_dim]\n",
    "\n",
    "# List to hold generated images and corresponding age buckets\n",
    "generated_images = []\n",
    "age_buckets_list = []\n",
    "\n",
    "# Upscale factor (e.g., upscale from 64x64 to 256x256)\n",
    "upscale_factor = 10  # Adjust as needed\n",
    "\n",
    "# Define age bucket labels (optional, for more informative titles)\n",
    "age_bucket_labels = {\n",
    "    1: '0-18 years',\n",
    "    2: '19-29 years',\n",
    "    3: '30-39 years',\n",
    "    4: '40-49 years',\n",
    "    5: '50-59 years',\n",
    "    6: '60+ years'\n",
    "}\n",
    "\n",
    "# 3. Generate Images with Different Age Conditions\n",
    "with torch.no_grad():\n",
    "    for age_bucket in range(1, 7):  # Age buckets from 1 to 6\n",
    "        # Create one-hot encoding for the current age bucket\n",
    "        age_one_hot = torch.zeros(1, 6).to(device)  # Shape: [1, 6]\n",
    "        age_one_hot[0, age_bucket - 1] = 1.0\n",
    "\n",
    "        # Combine age_one_hot and gender_condition to form the condition tensor\n",
    "        condition = torch.cat((age_one_hot, gender_condition.unsqueeze(0)), dim=1)  # Shape: [1, 7]\n",
    "\n",
    "        # Generate image using the generator\n",
    "        fake_img = generator(fixed_noise, condition)\n",
    "\n",
    "        # Upscale the generated image\n",
    "        fake_img_upscaled = F.interpolate(fake_img, scale_factor=upscale_factor, mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Append the generated image and age bucket to the lists\n",
    "        generated_images.append(fake_img_upscaled.cpu().squeeze(0))\n",
    "        age_buckets_list.append(age_bucket)\n",
    "\n",
    "# 4. Visualize the Generated Images\n",
    "num_images = len(generated_images)\n",
    "fig, axes = plt.subplots(1, num_images, figsize=(5 * num_images, 5))\n",
    "\n",
    "# Handle the case when there's only one image (axes would not be a list)\n",
    "if num_images == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    img = generated_images[idx]\n",
    "    age_bucket = age_buckets_list[idx]\n",
    "\n",
    "    # Unnormalize the image if necessary (assuming images are normalized to [-1, 1])\n",
    "    img = (img * 0.5) + 0.5  # Convert from [-1, 1] to [0, 1]\n",
    "\n",
    "    # Convert from Tensor to NumPy array and transpose dimensions for plotting\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    # Use age bucket label if defined, else just the bucket number\n",
    "    if age_bucket_labels:\n",
    "        ax.set_title(f'Age Bucket: {age_bucket_labels[age_bucket]}')\n",
    "    else:\n",
    "        ax.set_title(f'Age Bucket: {age_bucket}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Directories to save images\n",
    "generated_images_dir = 'generated_images'\n",
    "os.makedirs(generated_images_dir, exist_ok=True)\n",
    "\n",
    "# Number of images to generate\n",
    "num_images = 10000  # Adjust as needed\n",
    "batch_size = 64     # Adjust based on your GPU memory\n",
    "\n",
    "# Set generator to evaluation mode\n",
    "generator.eval()\n",
    "\n",
    "# Function to sample random conditions (adjusted to match your first code)\n",
    "def sample_random_conditions(batch_size):\n",
    "    # Sample random age buckets from 1 to 6\n",
    "    age_buckets = torch.randint(1, 7, (batch_size,))  # Age buckets: [1, 6]\n",
    "    \n",
    "    # Adjust age buckets to zero-based indexing for one-hot encoding\n",
    "    age_buckets_zero_indexed = age_buckets - 1  # Age buckets: [0, 5]\n",
    "    \n",
    "    # One-hot encode age buckets\n",
    "    age_buckets_one_hot = torch.nn.functional.one_hot(age_buckets_zero_indexed, num_classes=6).float()\n",
    "    \n",
    "    # Sample random genders (0 or 1)\n",
    "    genders = torch.randint(0, 2, (batch_size, 1)).float()\n",
    "    \n",
    "    # Combine conditions\n",
    "    conditions = torch.cat((age_buckets_one_hot, genders), dim=1)\n",
    "    return conditions\n",
    "\n",
    "# Generate and save images\n",
    "with torch.no_grad():\n",
    "    for i in range(0, num_images, batch_size):\n",
    "        current_batch_size = min(batch_size, num_images - i)\n",
    "        noise = torch.randn(current_batch_size, noise_dim).to(device)\n",
    "        conds = sample_random_conditions(current_batch_size).to(device)\n",
    "        fake_imgs = generator(noise, conds)\n",
    "        \n",
    "        # Denormalize images from [-1, 1] to [0, 1]\n",
    "        fake_imgs = (fake_imgs + 1) / 2\n",
    "        \n",
    "        for j in range(current_batch_size):\n",
    "            save_image(fake_imgs[j], os.path.join(generated_images_dir, f'gen_{i + j}.png'), normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "real_images_dir = 'real_images'\n",
    "os.makedirs(real_images_dir, exist_ok=True)\n",
    "\n",
    "# Number of real images to save (should match the number of generated images)\n",
    "num_real_images = num_images  # Ensure 'num_images' is defined elsewhere\n",
    "\n",
    "images_saved = 0\n",
    "\n",
    "# Assuming you have a DataLoader 'dataloader' for your dataset\n",
    "for real_imgs, _ in dataloader:\n",
    "    real_imgs = real_imgs.to(device)\n",
    "\n",
    "    # Denormalize images from [-1, 1] to [0, 1]\n",
    "    real_imgs = (real_imgs + 1) / 2\n",
    "\n",
    "    for i in range(real_imgs.size(0)):\n",
    "        if images_saved >= num_real_images:\n",
    "            break\n",
    "        save_image(real_imgs[i], os.path.join(real_images_dir, f'real_{images_saved}.png'), normalize=False)\n",
    "        images_saved += 1\n",
    "\n",
    "    if images_saved >= num_real_images:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_fid import fid_score\n",
    "\n",
    "# Compute FID\n",
    "fid_value = fid_score.calculate_fid_given_paths(\n",
    "    [real_images_dir, generated_images_dir],\n",
    "    batch_size=64,\n",
    "    device=device,\n",
    "    dims=2048,\n",
    ")\n",
    "print(f'FID Score: {fid_value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Custom Dataset for Generated Images\n",
    "class GeneratedImagesDataset(Dataset):\n",
    "    def __init__(self, images_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.image_files = sorted(os.listdir(images_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Transformation matching Inception model input\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.PILToTensor(),  # Converts to torch.uint8 in [0, 255] range\n",
    "])\n",
    "\n",
    "# Create DataLoader for generated images\n",
    "dataset = GeneratedImagesDataset(generated_images_dir, transform=transform)\n",
    "dataloader_IS = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize Inception Score metric\n",
    "inception_metric = InceptionScore().to(device)\n",
    "\n",
    "# Compute Inception Score\n",
    "for batch in dataloader_IS:\n",
    "    batch = batch.to(device)\n",
    "    inception_metric.update(batch)\n",
    "\n",
    "inception_score, std = inception_metric.compute()\n",
    "print(f'Inception Score: {inception_score:.4f} Â± {std:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
