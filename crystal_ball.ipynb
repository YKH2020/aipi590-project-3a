{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Project 3a: Advanced GAN Crystal Ball__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import datetime\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Torch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchgan.models as models\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "print(dir(models))\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())\n",
    "else:\n",
    "    print('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Download__ and __Extract__ the CelebA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl -L https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/wiki_crop.tar -o wiki_crop.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tar -xvzf wiki_crop.tar -C ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Load the Celeb-WIKI Dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file_path = './wiki_crop/wiki.mat'\n",
    "mat = scipy.io.loadmat(mat_file_path)\n",
    "\n",
    "mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from .mat file\n",
    "wiki = mat['wiki']\n",
    "\n",
    "full_path = wiki['full_path'][0][0][0]\n",
    "gender = wiki['gender'][0][0][0]\n",
    "dob = wiki['dob'][0][0][0]\n",
    "photo_taken = wiki['photo_taken'][0][0][0]\n",
    "face_location = wiki['face_location'][0][0][0]\n",
    "name = wiki['name'][0][0][0]\n",
    "face_score = wiki['face_score'][0][0][0]\n",
    "second_face_score = wiki['second_face_score'][0][0][0]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'full_path': full_path,\n",
    "    'gender': gender.flatten(),\n",
    "    'dob': dob.flatten(),\n",
    "    'photo_taken': photo_taken.flatten(),\n",
    "    'face_location': face_location.tolist(),\n",
    "    'name': name.flatten(),\n",
    "    'face_score': face_score.flatten(),\n",
    "    'second_face_score': second_face_score.flatten()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Cleaning__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -inf for face score means that the confidence of a face being detected in the image is virtually NONEXISTENT!\n",
    "num_neg_inf = (df['face_score'] == -np.inf).sum()\n",
    "print(num_neg_inf)\n",
    "df_filtered = df[df['face_score'] != -np.inf]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nans = df_filtered['second_face_score'].isna().sum()\n",
    "df_filtered = df_filtered[df_filtered['second_face_score'].isna()]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nans = df_filtered['gender'].isna().sum()\n",
    "df_filtered = df_filtered[df_filtered['gender'].notna()]\n",
    "num_nans = df_filtered['gender'].isna().sum()\n",
    "print(num_nans)\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.drop(columns=['second_face_score'])\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matlab_serial_to_year(serial_date):\n",
    "    # MATLAB's serial dates start from 0000-01-01, Python starts from 0001-01-01\n",
    "    origin = datetime.datetime(1, 1, 1)  # Using year 1\n",
    "    delta = datetime.timedelta(days=int(serial_date) - 366)  # Subtract 366 to adjust MATLAB's start year (0)\n",
    "    return (origin + delta).year\n",
    "\n",
    "# Assuming your cleaned DataFrame is named 'df'\n",
    "def get_age_bucket(age):\n",
    "    if age <= 18:\n",
    "        return 0\n",
    "    elif 19 <= age <= 29:\n",
    "        return 1\n",
    "    elif 30 <= age <= 39:\n",
    "        return 2\n",
    "    elif 40 <= age <= 49:\n",
    "        return 3\n",
    "    elif 50 <= age <= 59:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "final_df = df_filtered.copy()\n",
    "final_df['dob'] = df_filtered['dob'].apply(matlab_serial_to_year)\n",
    "\n",
    "# Add another feature\n",
    "final_df['age'] = final_df['photo_taken'] - final_df['dob']\n",
    "final_df = final_df.drop(columns=['dob', 'photo_taken'])\n",
    "\n",
    "# Assign age bucket to each row\n",
    "final_df['age_bucket'] = final_df['age'].apply(get_age_bucket)\n",
    "\n",
    "# Add './wiki_crop/' prefix to the full_path column to get the correct paths\n",
    "final_df['full_path'] = final_df['full_path'].apply(lambda x: f\"./wiki_crop/{x[0]}\")\n",
    "\n",
    "# Convert gender to int\n",
    "final_df['gender'] = final_df['gender'].astype(int)\n",
    "\n",
    "# Check the updated DataFrame\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['face_location'] = final_df['face_location'].apply(lambda x: x[0].tolist() if isinstance(x, np.ndarray) and x.ndim == 2 else x)\n",
    "final_df['name'] = final_df['name'].apply(lambda x: x[0] if isinstance(x, np.ndarray) and x.ndim == 1 else x)\n",
    "\n",
    "print(final_df.dtypes)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Preprocessing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize cropped faces to 64x64\n",
    "    transforms.ToTensor(),  # Convert PIL Image to Tensor\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceAgingDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing image paths, gender, and labels.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the row corresponding to the index\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Load the image\n",
    "        img = Image.open(row[\"full_path\"]).convert(\"RGB\")\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Get the age bucket (convert to zero-indexed for PyTorch)\n",
    "        age_bucket = row[\"age_bucket\"] - 1\n",
    "\n",
    "        # Get the gender\n",
    "        gender = row[\"gender\"]\n",
    "\n",
    "        return img, age_bucket, gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = FaceAgingDataset(final_df, transform=transform)\n",
    "dataloader = DataLoader(\n",
    "    FaceAgingDataset(final_df, transform=transform), \n",
    "    batch_size=64, \n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Modeling__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),  # Output: [batch_size, 64, 32, 32]\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),  # Output: [batch_size, 128, 16, 16]\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),  # Output: [batch_size, 256, 8, 8]\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),  # Output: [batch_size, 512, 4, 4]\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False),  # Output: [batch_size, 1, 1, 1]\n",
    "            nn.Sigmoid()  # Scalar probability\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input).view(-1)  # Flatten to [batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, condition_dim, additional_features=1, output_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(noise_dim + condition_dim + additional_features, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 3 * 64 * 64),  # Generate flattened 64x64 image\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, condition, additional_features):\n",
    "        # Concatenate noise, condition, and additional features\n",
    "        x = torch.cat((noise, condition, additional_features), dim=1)\n",
    "        img = self.fc(x)\n",
    "        return img.view(-1, 3, 64, 64)  # Reshape to batch_size x 3 x 64 x 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define dimensions for noise and condition\n",
    "noise_dim = 100  # Latent noise dimension\n",
    "condition_dim = 6  # Number of age buckets\n",
    "\n",
    "# Initialize models\n",
    "discriminator = Discriminator().to(device)\n",
    "generator = Generator(noise_dim=noise_dim, condition_dim=condition_dim).to(device)\n",
    "\n",
    "# Initialize optimizers\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_train_step(real_data, fake_data):\n",
    "    d_optimizer.zero_grad()\n",
    "\n",
    "    # Train on real data\n",
    "    prediction_real = discriminator(real_data)\n",
    "    real_labels = torch.ones(real_data.size(0)).to(device)\n",
    "    error_real = loss_fn(prediction_real, real_labels)\n",
    "    error_real.backward()\n",
    "\n",
    "    # Train on fake data\n",
    "    prediction_fake = discriminator(fake_data.detach())  # Detach to avoid updating generator\n",
    "    fake_labels = torch.zeros(fake_data.size(0)).to(device)\n",
    "    error_fake = loss_fn(prediction_fake, fake_labels)\n",
    "    error_fake.backward()\n",
    "\n",
    "    d_optimizer.step()\n",
    "\n",
    "    return error_real + error_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train_step(fake_data):\n",
    "    g_optimizer.zero_grad()\n",
    "\n",
    "    # Generate predictions\n",
    "    prediction = discriminator(fake_data)\n",
    "    real_labels = torch.ones(fake_data.size(0)).to(device)\n",
    "    error = loss_fn(prediction, real_labels)  # Want discriminator to think fake is real\n",
    "    error.backward()\n",
    "\n",
    "    g_optimizer.step()\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 64\n",
    "noise_dim = 100\n",
    "condition_dim = 6  # Number of age buckets\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataloader = DataLoader(FaceAgingDataset(final_df, transform=transform), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for real_images, age_buckets, gender in dataloader:\n",
    "        real_images, age_buckets, gender = real_images.to(device), age_buckets.to(device), gender.to(device)\n",
    "\n",
    "        # One-hot encode the age buckets for conditional input\n",
    "        age_conditions = torch.eye(condition_dim).to(device)[age_buckets]\n",
    "\n",
    "        # Combine age conditions with gender\n",
    "        gender = gender.unsqueeze(1)  # Expand gender dimensions to match\n",
    "        combined_conditions = torch.cat((age_conditions, gender), dim=1)\n",
    "\n",
    "        # Generate fake images\n",
    "        noise = torch.randn(real_images.size(0), noise_dim).to(device)\n",
    "        fake_images = generator(noise, age_conditions, gender)  # Pass age_conditions and gender separately\n",
    "\n",
    "        # Train Discriminator\n",
    "        d_loss = discriminator_train_step(real_images, fake_images)\n",
    "\n",
    "        # Train Generator\n",
    "        noise = torch.randn(real_images.size(0), noise_dim).to(device)\n",
    "        fake_images = generator(noise, age_conditions, gender)  # Pass age_conditions and gender separately\n",
    "        g_loss = generator_train_step(fake_images)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = \"output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save sample images\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        save_image(fake_images.data[:25], f\"{output_dir}/epoch_{epoch + 1}.png\", nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_age_buckets(generator, dataset, num_age_buckets=6):\n",
    "    generator.eval()  # Set generator to evaluation mode\n",
    "\n",
    "    # Select a random image from the dataset\n",
    "    idx = torch.randint(0, len(dataset), (1,)).item()\n",
    "    original_img, age_bucket, gender = dataset[idx]  # Extract all values from dataset\n",
    "    original_img = original_img.to(device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Generate noise for the generator\n",
    "    noise_dim = 100\n",
    "    fixed_noise = torch.randn(1, noise_dim).to(device)  # Fixed noise for consistency\n",
    "\n",
    "    # Create conditional inputs for all age buckets\n",
    "    generated_images = []\n",
    "    for age_bucket in range(num_age_buckets):\n",
    "        condition = torch.zeros(1, num_age_buckets).to(device)\n",
    "        condition[0, age_bucket] = 1  # One-hot encode the age bucket\n",
    "\n",
    "        # Add gender to condition\n",
    "        gender_condition = torch.tensor([[gender]], device=device)  # Convert to tensor\n",
    "        combined_condition = torch.cat((condition, gender_condition), dim=1)\n",
    "\n",
    "        # Pass noise and condition to the generator\n",
    "        fake_img = generator(fixed_noise, condition, gender_condition).detach().cpu()\n",
    "        generated_images.append(fake_img.squeeze())\n",
    "\n",
    "    # Plot the original image and the generated variations\n",
    "    fig, axes = plt.subplots(1, num_age_buckets + 1, figsize=(15, 5))\n",
    "    axes[0].imshow(original_img.squeeze().cpu().permute(1, 2, 0) * 0.5 + 0.5)  # De-normalize\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    for i, fake_img in enumerate(generated_images):\n",
    "        axes[i + 1].imshow(fake_img.permute(1, 2, 0) * 0.5 + 0.5)  # De-normalize\n",
    "        axes[i + 1].set_title(f\"Age Bucket {i + 1}\")\n",
    "        axes[i + 1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_age_buckets(generator, FaceAgingDataset(final_df, transform=transform))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
